{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07c1fc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proof of concept implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93bdcd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_vit import ViT\n",
    "import json\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93c6f4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "#img = Image.open('img.jpg')\n",
    "\n",
    "#transform = transforms.Compose(\n",
    "#    [transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "#batch_size = 1\n",
    "\n",
    "# testset = torchvision.datasets.ImageNet(root='./data', train=False, download=True, transform=transform)\n",
    "#testset = torchvision.datasets.FakeData(100,(3,224,224), 10, transform=transform)\n",
    "\n",
    "#testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "#classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8d5f310",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('img.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54b113b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#import numpy as np\n",
    "\n",
    "\n",
    "# Show image\n",
    "#def imshow(img):\n",
    "#    img = img / 2 + 0.5     # unnormalize\n",
    "#    npimg = img.numpy()\n",
    "#    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "#    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "#dataiter = iter(testloader)\n",
    "#image, label = dataiter.next()\n",
    "\n",
    "# show images\n",
    "#imshow(torchvision.utils.make_grid(image))\n",
    "# print labels\n",
    "#print(' '.join('%5s' % classes[label[j]] for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d3e3ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights.\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained Vision Transformer\n",
    "model = ViT('B_16_imagenet1k', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b40c3b32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/lautenschlager/.cache/torch/hub/ultralytics_yolov5_master\n",
      "Fusing layers... \n",
      "Model Summary: 224 layers, 7266973 parameters, 0 gradients\n",
      "Adding AutoShape... \n",
      "YOLOv5 ðŸš€ 2021-5-23 torch 1.8.1 CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoShape already enabled, skipping... \n",
      "image 1/1: 563x845 1 bird\n",
      "Speed: 10.6ms pre-process, 186.8ms inference, 1.3ms NMS per image at shape (1, 3, 448, 640)\n"
     ]
    }
   ],
   "source": [
    "# Object detection\n",
    "detector_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True).autoshape()\n",
    "results = detector_model(img, size=640)\n",
    "results.print()\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fd6d539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bounding box is: 253 87 587 541\n"
     ]
    }
   ],
   "source": [
    "# Get bounding box coordinates from detection\n",
    "x1 = int(results.xyxy[0][0][0])\n",
    "y1 = int(results.xyxy[0][0][1])\n",
    "x2 = int(results.xyxy[0][0][2])\n",
    "y2 = int(results.xyxy[0][0][3])\n",
    "bbox_points=[x1, y1, x2, y2]\n",
    "print('bounding box is:', x1, y1, x2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32ff4106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.Image.Image"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show cropped image (bounding box)\n",
    "cropped_img = img.crop((x1, y1, x2, y2))\n",
    "cropped_img.show()\n",
    "type(cropped_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3d644ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess image for transformer\n",
    "tfms = transforms.Compose([transforms.Resize(model.image_size), transforms.ToTensor(), transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),])\n",
    "preprocessed_image = tfms(cropped_img).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc1fd5ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rgb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f3364e689dd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0mdlab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegmentation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeeplabv3_resnet101\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m \u001b[0msegment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdlab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcropped_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_orig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-f3364e689dd2>\u001b[0m in \u001b[0;36msegment\u001b[0;34m(net, image, show_orig, dev)\u001b[0m\n\u001b[1;32m     91\u001b[0m   \u001b[0mom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m   \u001b[0mrgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_segmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-f3364e689dd2>\u001b[0m in \u001b[0;36mdecode_segmap\u001b[0;34m(image, source, nc)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;31m# Create a background array to hold white pixels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;31m# with the same size as RGB output map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m   \u001b[0mbackground\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;31m# Convert uint8 to float\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rgb' is not defined"
     ]
    }
   ],
   "source": [
    "# Segmentation\n",
    "# Code adapted from https://github.com/spmallick/learnopencv/blob/master/app-seperation-semseg/Background-Removal.py\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "# %matplotlib.use('TkAgg')\n",
    "\n",
    "# Apply the transformations needed\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Define the helper function\n",
    "def decode_segmap(image, source, nc=21):\n",
    "  \n",
    "#  label_colors = np.array([(0, 0, 0),  # 0=background\n",
    "#               # 1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle\n",
    "#               (128, 0, 0), (0, 128, 0), (128, 128, 0), (0, 0, 128), (128, 0, 128),\n",
    "#               # 6=bus, 7=car, 8=cat, 9=chair, 10=cow\n",
    "#               (0, 128, 128), (128, 128, 128), (64, 0, 0), (192, 0, 0), (64, 128, 0),\n",
    "#               # 11=dining table, 12=dog, 13=horse, 14=motorbike, 15=person\n",
    "#               (192, 128, 0), (64, 0, 128), (192, 0, 128), (64, 128, 128), (192, 128, 128),\n",
    "#               # 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv/monitor\n",
    "#               (0, 64, 0), (128, 64, 0), (0, 192, 0), (128, 192, 0), (0, 64, 128)])\n",
    "#\n",
    "#  r = np.zeros_like(image).astype(np.uint8)\n",
    "#  g = np.zeros_like(image).astype(np.uint8)\n",
    "#  b = np.zeros_like(image).astype(np.uint8)\n",
    "#   \n",
    "#\n",
    "#  for l in range(0, nc):\n",
    "#    idx = image == l\n",
    "#    r[idx] = label_colors[l, 0]\n",
    "#    g[idx] = label_colors[l, 1]\n",
    "#    b[idx] = label_colors[l, 2]\n",
    "# \n",
    "#  rgb = np.stack([r, g, b], axis=2)\n",
    "\n",
    "  # Load the foreground input image \n",
    "  foreground = source\n",
    "\n",
    "  # Change the color of foreground image to RGB \n",
    "  # and resize image to match shape of R-band in RGB output map\n",
    "  #foreground = cv2.cvtColor(foreground, cv2.COLOR_BGR2RGB)\n",
    "  \n",
    "  #foreground = cv2.resize(np.array(foreground),(r.shape[1],r.shape[0]))\n",
    "\n",
    "  # Create a background array to hold white pixels\n",
    "  # with the same size as RGB output map\n",
    "  background = 255 * np.ones_like(rgb).astype(np.uint8)\n",
    "\n",
    "  # Convert uint8 to float\n",
    "  foreground = foreground.astype(float)\n",
    "  background = background.astype(float)\n",
    "\n",
    "  # Create a binary mask of the RGB output map using the threshold value 0\n",
    "  th, alpha = cv2.threshold(np.array(rgb),0,255, cv2.THRESH_BINARY)\n",
    "\n",
    "  # Apply a slight blur to the mask to soften edges\n",
    "  alpha = cv2.GaussianBlur(alpha, (7,7),0)\n",
    "\n",
    "  # Normalize the alpha mask to keep intensity between 0 and 1\n",
    "  alpha = alpha.astype(float)/255\n",
    "\n",
    "  # Multiply the foreground with the alpha matte\n",
    "  foreground = cv2.multiply(alpha, foreground)  \n",
    "  \n",
    "  # Multiply the background with ( 1 - alpha )\n",
    "  background = cv2.multiply(1.0 - alpha, background)  \n",
    "  \n",
    "  # Add the masked foreground and background\n",
    "  outImage = cv2.add(foreground, background)\n",
    "\n",
    "  # Return a normalized output image for display\n",
    "  return outImage/255\n",
    "\n",
    "def segment(net, image, show_orig=True, dev='cpu'):\n",
    "  img = image\n",
    "  if show_orig: plt.imshow(img); plt.axis('off'); plt.show()\n",
    "  # Comment the Resize and CenterCrop for better inference results\n",
    "  trf = T.Compose([T.Resize(450), \n",
    "                   #T.CenterCrop(224), \n",
    "                   T.ToTensor(), \n",
    "                   T.Normalize(mean = [0.485, 0.456, 0.406], \n",
    "                               std = [0.229, 0.224, 0.225])])\n",
    "  inp = trf(img).unsqueeze(0).to(dev)\n",
    "  out = net.to(dev)(inp)['out']\n",
    "  om = torch.argmax(out.squeeze(), dim=0).detach().cpu().numpy()\n",
    "  \n",
    "  rgb = decode_segmap(om, img)\n",
    "    \n",
    "  plt.imshow(rgb); plt.axis('off'); plt.show()\n",
    "\n",
    "  plt.savefig(fname='seg_result')\n",
    "  \n",
    "dlab = models.segmentation.deeplabv3_resnet101(pretrained=1).eval()\n",
    "\n",
    "segment(dlab, cropped_img, show_orig=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5e1045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b75bba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load class labels\n",
    "labels_map = json.load(open('labels_map.txt'))\n",
    "labels_map = [labels_map[str(i)] for i in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21fadeac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "[24] great grey owl, great gray owl, Strix nebulosa                              (99.98%)\n",
      "[21] kite                                                                        (0.00%)\n",
      "[798] slide rule, slipstick                                                       (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# Classify\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(preprocessed_image).squeeze(0)\n",
    "print('-----')  \n",
    "for idx in torch.topk(outputs, k=3).indices.tolist():\n",
    "    prob = torch.softmax(outputs, -1)[idx].item()\n",
    "    print('[{idx}] {label:<75} ({p:.2f}%)'.format(idx=idx, label=labels_map[idx], p=prob*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6f0ec1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
